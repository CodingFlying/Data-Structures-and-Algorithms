# 排序优化

## <font color = blue>内容概要</font>

1. 如何实现一个通用的、高性能的排序函数？

   


## <font color = blue> 遗留问题</font>

1. 



### 如何选择合适的排序算法

+ 算法总结

  ![](25.jpg)

+ 算法比较

  + 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。
  + 如果对小规模数据进行排序，可以选择时间复杂度是$ O(n^2)$ 的算法；如果对大规模数据进行排序，时间复杂度是 $O(nlogn)$ 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是  $O(nlogn)$ 的排序算法来实现排序函数。
  + 时间复杂度是  $O(nlogn)$  的排序算法不止一个，有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。
  + 使用归并排序的情况其实并不多。快排在最坏情况下的时间复杂度是 $O(n^2)$，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是  $O(nlogn)$ ，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？归并排序并不是原地排序算法，空间复杂度是 O(n)。所以，粗略点、夸张点讲，如果要排序 100MB 的数据，除了数据本身占用的内存之外，排序算法还要额外再占用 100MB 的内存空间，空间耗费就翻倍了。
  + 快速排序比较适合来实现排序函数，但是，快速排序在最坏情况下的时间复杂度是 O(n2)，如何来解决这个“复杂度恶化”的问题呢？



### 如何优化快速排序？

+ 我们先来看下，为什么最坏情况下快速排序的时间复杂度是 $O(n^2)$ 呢？
  + 如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为  $O(n^2)$ 。**实际上，这种  $O(n^2)$ 时间复杂度出现的主要原因还是因为我们分区点选的不够合理**。
  + 那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？
    + 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。
    + 如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是  $O(n^2)$ 。为了提高排序算法的性能，也要尽可能地让每次分区都比较平均。
+ 两个比较常用、比较简单的分区算法
  + 三数取中法
    + 从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。
    + 这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。
    + 但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。
  + 随机法
    + 随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。
    + 这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的  $O(n^2)$ 的情况，出现的可能性不大。
+ 警惕快速排序堆栈溢出
  + 快速排序是用递归来实现的，递归要警惕堆栈溢出。
  + 为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，有两种解决办法：
    + 第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。
    + 第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。









